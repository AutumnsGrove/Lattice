---
title: What is Petal?
description: Grove's privacy-first image content moderation system
category: help
section: how-it-works
lastUpdated: '2026-01-28'
keywords:
  - petal
  - image moderation
  - content safety
  - privacy
  - photo moderation
order: 42
---

# What is Petal?

Words aren't the only thing that needs protection. Images do too.

Petal is Grove's image moderation system—the visual counterpart to Thorn. It reviews uploaded images and AI-generated visuals for harmful content, using the same privacy-first principles: no human surveillance, immediate deletion after review, zero data retention.

## Why Petal exists

Image moderation is even harder than text moderation. Visual content can be manipulated, taken out of context, or generated to look like something it isn't. Many platforms either over-moderate (blocking legitimate art) or under-moderate (letting harmful content through).

Petal aims for balance. Four layers of protection catch different types of problems:

1. **CSAM detection** — Mandatory, no exceptions. This is required by law and essential for protecting children.
2. **Content classification** — AI analyzes images for nudity, violence, and other policy violations.
3. **Sanity checks** — Application-specific validation (is this actually what it claims to be?).
4. **Human escalation** — For edge cases only, with strict privacy protocols.

Like Thorn, Petal protects without surveillance. Your images aren't seen by human reviewers unless absolutely necessary, and content is deleted immediately after analysis.

## How it works

When you upload an image anywhere in Grove—profile photos, blog posts, anywhere—Petal reviews it automatically.

**Layer by layer.** The image passes through multiple checks. Each layer catches different problems. Only if every layer passes does the image go live.

**Clear rejections.** If an image is blocked, you get a user-friendly explanation, not scary legalese. Often you can try again with a different image.

**Legal compliance.** Some things are non-negotiable. CSAM detection is mandatory everywhere, and violations are reported to authorities as required by law.

**Privacy preserved.** Images are processed with zero data retention. They're deleted immediately after review, never stored or used for training.

## What this means for you

**Images are reviewed automatically.** Like text, this happens on every upload. Most images pass without issue.

**Quick and transparent.** If an image is rejected, you'll know why. Rejection messages are helpful, not punitive.

**No human eyes (usually).** Automated systems handle the vast majority of cases. Human review only happens in rare edge cases, with strict protocols.

**Same privacy standards.** Everything about Grove's privacy commitment applies to images too.

## Related

- [What is Thorn?](/knowledge/help/what-is-thorn) — Text content moderation
- [Adding images and media](/knowledge/help/adding-images-and-media)
- [What is Zero Data Retention?](/knowledge/help/what-is-zdr)
- [Petal Specification](/knowledge/specs/petal-spec)
- [Grove Workshop → Petal](/workshop#tool-petal)

---

*Petals close to protect what's precious. Layer by layer, gentle but vigilant.*
